{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple LangGraph Chatbot",
   "id": "c61b23d388229a29"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T08:38:26.540756Z",
     "start_time": "2025-11-15T08:38:07.265960Z"
    }
   },
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "MODEL = \"llama2\"\n",
    "MODEL_PROVIDER = \"ollama\"\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=f\"{MODEL_PROVIDER}:{MODEL}\"\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "user_input = input(\"> \")\n",
    "state = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]})\n",
    "\n",
    "print(state[\"messages\"])\n",
    "print(state[\"messages\"][-1].content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='what is the square of 13?', additional_kwargs={}, response_metadata={}, id='8ab90530-1da3-4358-bf7b-5a72a6f0b087'), AIMessage(content='The square of 13 is 169.', additional_kwargs={}, response_metadata={'model': 'llama2', 'created_at': '2025-11-15T08:38:26.537566Z', 'done': True, 'done_reason': 'stop', 'total_duration': 724313125, 'load_duration': 40833459, 'prompt_eval_count': 29, 'prompt_eval_duration': 275100708, 'eval_count': 13, 'eval_duration': 384804248, 'model_name': 'llama2', 'model_provider': 'ollama'}, id='lc_run--1f06e819-6392-49e1-9fcc-b4ff8aa6c6a8-0', usage_metadata={'input_tokens': 29, 'output_tokens': 13, 'total_tokens': 42})]\n",
      "The square of 13 is 169.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Complex LangGraph Chatbot",
   "id": "c467f83630f57c20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Annotated, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.chat_models import init_chat_model\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "MODEL = \"llama2\"\n",
    "MODEL_PROVIDER = \"ollama\"\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=f\"{MODEL_PROVIDER}:{MODEL}\"\n",
    ")\n",
    "\n",
    "\n",
    "class MessageClassifier(BaseModel):\n",
    "    message_type: Literal[\"emotional\", \"logical\"] = Field(\n",
    "        ...,\n",
    "        description=\"Classify if the message requires an emotional (therpaist) or logical response.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    message_type: str | None\n",
    "\n",
    "\n",
    "def classify_message(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    classifier_llm = llm.with_structured_output(MessageClassifier)\n",
    "\n",
    "    result = classifier_llm.invoke([\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Classify the user message as either:\n",
    "            - 'emotional': if it asks for emotional support, therapy, deals with feelings, or personal problems\n",
    "            - 'logical': if it asks for facts, information, logical analysis, or practical solutions\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": last_message.content,\n",
    "        }\n",
    "    ])\n",
    "    return {\"message_type\": result.message_type}\n",
    "\n",
    "\n",
    "def router(state: State):\n",
    "    message_type = state.get(\"message_type\", \"logical\")\n",
    "    if message_type == \"emotional\":\n",
    "        return {\"next\": \"therapist\"}\n",
    "    return {\"next\": \"logical\"}\n",
    "\n",
    "\n",
    "def therapist_agent(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a compassionate therapist. Focus on the emotional aspects of the user's message.\n",
    "            Show empathy, validate their feelings, and help them process their emotions.\n",
    "            Ask thoughtful questions to help them explore their feelings more deeply.\n",
    "            Avoid giving logical solutions unless explicitly asked.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": last_message.content\n",
    "        },\n",
    "    ]\n",
    "    reply = llm.invoke(messages)\n",
    "    return {\"messages\": [{\"role\": \"assistant\", \"content\": reply.content}]}\n",
    "\n",
    "\n",
    "def logical_agent(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a purely logical assistant. Focus only on facts and information.\n",
    "            Provide clear, concise answers based on logic and evidence.\n",
    "            Do not address emotions or provide emotional support.\n",
    "            Be direct and straightforward in your responses.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": last_message.content\n",
    "        },\n",
    "    ]\n",
    "    reply = llm.invoke(messages)\n",
    "    return {\"messages\": [{\"role\": \"assistant\", \"content\": reply.content}]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"classifier\", classify_message)\n",
    "graph_builder.add_node(\"router\", router)\n",
    "graph_builder.add_node(\"therapist\", therapist_agent)\n",
    "graph_builder.add_node(\"logical\", logical_agent)\n",
    "\n",
    "graph_builder.add_edge(START, \"classifier\")\n",
    "graph_builder.add_edge(\"classifier\", \"router\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state.get(\"next\"),\n",
    "    {\"therapist\": \"therapist\", \"logical\": \"logical\"},\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"therapist\", END)\n",
    "graph_builder.add_edge(\"logical\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "def run_chatbot():\n",
    "    state = {\"messages\": [], \"message_type\": None}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input == \"exit\":\n",
    "            print(\"Bye\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"] = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "\n",
    "        state = graph.invoke(state)\n",
    "\n",
    "        if state.get(\"messages\") and len(state.get(\"messages\")) > 0:\n",
    "            last_message = state[\"messages\"][-1]\n",
    "            print(f\"Assistant: {last_message.content}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()\n",
    "    "
   ],
   "id": "1898e582c072f13a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
